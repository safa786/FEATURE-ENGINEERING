{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**This notebook is an exercise in the [Feature Engineering](https://www.kaggle.com/learn/feature-engineering) course.  You can reference the tutorial at [this link](https://www.kaggle.com/ryanholbrook/target-encoding).**\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"# Introduction #\n\nIn this exercise, you'll apply target encoding to features in the [*Ames*](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data) dataset.\n\nRun this cell to set everything up!","metadata":{}},{"cell_type":"code","source":"# Setup feedback system\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.feature_engineering_new.ex6 import *\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport warnings\nfrom category_encoders import MEstimateEncoder\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import XGBRegressor\n\n# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=14,\n    titlepad=10,\n)\nwarnings.filterwarnings('ignore')\n\n\ndef score_dataset(X, y, model=XGBRegressor()):\n    # Label encoding for categoricals\n    for colname in X.select_dtypes([\"category\", \"object\"]):\n        X[colname], _ = X[colname].factorize()\n    # Metric for Housing competition is RMSLE (Root Mean Squared Log Error)\n    score = cross_val_score(\n        model, X, y, cv=5, scoring=\"neg_mean_squared_log_error\",\n    )\n    score = -1 * score.mean()\n    score = np.sqrt(score)\n    return score\n\n\ndf = pd.read_csv(\"../input/fe-course-data/ames.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-06-18T15:13:18.236732Z","iopub.execute_input":"2021-06-18T15:13:18.237128Z","iopub.status.idle":"2021-06-18T15:13:18.297166Z","shell.execute_reply.started":"2021-06-18T15:13:18.2371Z","shell.execute_reply":"2021-06-18T15:13:18.296289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-------------------------------------------------------------------------------\n\nFirst you'll need to choose which features you want to apply a target encoding to. Categorical features with a large number of categories are often good candidates. Run this cell to see how many categories each categorical feature in the *Ames* dataset has.","metadata":{}},{"cell_type":"code","source":"df.select_dtypes([\"object\"]).nunique()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T15:13:18.299018Z","iopub.execute_input":"2021-06-18T15:13:18.299285Z","iopub.status.idle":"2021-06-18T15:13:18.34475Z","shell.execute_reply.started":"2021-06-18T15:13:18.29926Z","shell.execute_reply":"2021-06-18T15:13:18.343979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We talked about how the M-estimate encoding uses smoothing to improve estimates for rare categories. To see how many times a category occurs in the dataset, you can use the `value_counts` method. This cell shows the counts for `SaleType`, but you might want to consider others as well.","metadata":{}},{"cell_type":"code","source":"df[\"SaleType\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T15:13:18.346212Z","iopub.execute_input":"2021-06-18T15:13:18.346608Z","iopub.status.idle":"2021-06-18T15:13:18.356319Z","shell.execute_reply.started":"2021-06-18T15:13:18.346568Z","shell.execute_reply":"2021-06-18T15:13:18.354902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1) Choose Features for Encoding\n\nWhich features did you identify for target encoding? After you've thought about your answer, run the next cell for some discussion.","metadata":{}},{"cell_type":"code","source":"# View the solution (Run this cell to receive credit!)\nq_1.check()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T15:13:18.357931Z","iopub.execute_input":"2021-06-18T15:13:18.358446Z","iopub.status.idle":"2021-06-18T15:13:18.36916Z","shell.execute_reply.started":"2021-06-18T15:13:18.358404Z","shell.execute_reply":"2021-06-18T15:13:18.368378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-------------------------------------------------------------------------------\n\nNow you'll apply a target encoding to your choice of feature. As we discussed in the tutorial, to avoid overfitting, we need to fit the encoder on data heldout from the training set. Run this cell to create the encoding and training splits:","metadata":{}},{"cell_type":"code","source":"# Encoding split\nX_encode = df.sample(frac=0.20, random_state=0)\ny_encode = X_encode.pop(\"SalePrice\")\n\n# Training split\nX_pretrain = df.drop(X_encode.index)\ny_train = X_pretrain.pop(\"SalePrice\")","metadata":{"lines_to_next_cell":2,"execution":{"iopub.status.busy":"2021-06-18T15:13:18.371937Z","iopub.execute_input":"2021-06-18T15:13:18.372252Z","iopub.status.idle":"2021-06-18T15:13:18.384894Z","shell.execute_reply.started":"2021-06-18T15:13:18.372225Z","shell.execute_reply":"2021-06-18T15:13:18.383904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2) Apply M-Estimate Encoding\n\nApply a target encoding to your choice of categorical features. Also choose a value for the smoothing parameter `m` (any value is okay for a correct answer).","metadata":{}},{"cell_type":"code","source":"# YOUR CODE HERE: Create the MEstimateEncoder\n# Choose a set of features to encode and a value for m\nencoder = MEstimateEncoder(\n    cols=[\"Neighborhood\"],\n    m=1.0,\n)\n\n\n\n# Fit the encoder on the encoding split\nencoder.fit(X_encode, y_encode)\n\n# Encode the training split\nX_train = encoder.transform(X_pretrain, y_train)\n\n\n# Check your answer\nq_2.check()","metadata":{"lines_to_next_cell":0,"execution":{"iopub.status.busy":"2021-06-18T15:14:22.200733Z","iopub.execute_input":"2021-06-18T15:14:22.201065Z","iopub.status.idle":"2021-06-18T15:14:22.274489Z","shell.execute_reply.started":"2021-06-18T15:14:22.201036Z","shell.execute_reply":"2021-06-18T15:14:22.27334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lines below will give you a hint or solution code\n#q_2.hint()\n#q_2.solution()","metadata":{"lines_to_next_cell":0,"execution":{"iopub.status.busy":"2021-06-18T15:14:40.28515Z","iopub.execute_input":"2021-06-18T15:14:40.285517Z","iopub.status.idle":"2021-06-18T15:14:40.289104Z","shell.execute_reply.started":"2021-06-18T15:14:40.28549Z","shell.execute_reply":"2021-06-18T15:14:40.288089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you'd like to see how the encoded feature compares to the target, you can run this cell:","metadata":{}},{"cell_type":"code","source":"feature = encoder.cols\n\nplt.figure(dpi=90)\nax = sns.distplot(y_train, kde=True, hist=False)\nax = sns.distplot(X_train[feature], color='r', ax=ax, hist=True, kde=False, norm_hist=True)\nax.set_xlabel(\"SalePrice\");","metadata":{"execution":{"iopub.status.busy":"2021-06-18T15:14:43.530939Z","iopub.execute_input":"2021-06-18T15:14:43.531474Z","iopub.status.idle":"2021-06-18T15:14:43.879899Z","shell.execute_reply.started":"2021-06-18T15:14:43.531431Z","shell.execute_reply":"2021-06-18T15:14:43.878993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the distribution plots, does it seem like the encoding is informative?\n\nAnd this cell will show you the score of the encoded set compared to the original set:","metadata":{}},{"cell_type":"code","source":"X = df.copy()\ny = X.pop(\"SalePrice\")\nscore_base = score_dataset(X, y)\nscore_new = score_dataset(X_train, y_train)\n\nprint(f\"Baseline Score: {score_base:.4f} RMSLE\")\nprint(f\"Score with Encoding: {score_new:.4f} RMSLE\")","metadata":{"execution":{"iopub.status.busy":"2021-06-18T15:14:47.079314Z","iopub.execute_input":"2021-06-18T15:14:47.0797Z","iopub.status.idle":"2021-06-18T15:14:51.910994Z","shell.execute_reply.started":"2021-06-18T15:14:47.079671Z","shell.execute_reply":"2021-06-18T15:14:51.910145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Do you think that target encoding was worthwhile in this case? Depending on which feature or features you chose, you may have ended up with a score significantly worse than the baseline. In that case, it's likely the extra information gained by the encoding couldn't make up for the loss of data used for the encoding.","metadata":{}},{"cell_type":"markdown","source":"-------------------------------------------------------------------------------\n\nIn this question, you'll explore the problem of overfitting with target encodings. This will illustrate this importance of training fitting target encoders on data held-out from the training set.\n\nSo let's see what happens when we fit the encoder and the model on the *same* dataset. To emphasize how dramatic the overfitting can be, we'll mean-encode a feature that should have no relationship with `SalePrice`, a count: `0, 1, 2, 3, 4, 5, ...`.","metadata":{}},{"cell_type":"code","source":"# Try experimenting with the smoothing parameter m\n# Try 0, 1, 5, 50\nm = 0\n\nX = df.copy()\ny = X.pop('SalePrice')\n\n# Create an uninformative feature\nX[\"Count\"] = range(len(X))\nX[\"Count\"][1] = 0  # actually need one duplicate value to circumvent error-checking in MEstimateEncoder\n\n# fit and transform on the same dataset\nencoder = MEstimateEncoder(cols=\"Count\", m=m)\nX = encoder.fit_transform(X, y)\n\n# Results\nscore =  score_dataset(X, y)\nprint(f\"Score: {score:.4f} RMSLE\")","metadata":{"execution":{"iopub.status.busy":"2021-06-18T15:14:52.106764Z","iopub.execute_input":"2021-06-18T15:14:52.10742Z","iopub.status.idle":"2021-06-18T15:14:54.760581Z","shell.execute_reply.started":"2021-06-18T15:14:52.107387Z","shell.execute_reply":"2021-06-18T15:14:54.759867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Almost a perfect score!","metadata":{}},{"cell_type":"code","source":"plt.figure(dpi=90)\nax = sns.distplot(y, kde=True, hist=False)\nax = sns.distplot(X[\"Count\"], color='r', ax=ax, hist=True, kde=False, norm_hist=True)\nax.set_xlabel(\"SalePrice\");","metadata":{"execution":{"iopub.status.busy":"2021-06-18T15:14:59.255457Z","iopub.execute_input":"2021-06-18T15:14:59.255966Z","iopub.status.idle":"2021-06-18T15:14:59.583904Z","shell.execute_reply.started":"2021-06-18T15:14:59.255932Z","shell.execute_reply":"2021-06-18T15:14:59.583125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And the distributions are almost exactly the same, too.\n\n# 3) Overfitting with Target Encoders\n\nBased on your understanding of how mean-encoding works, can you explain how XGBoost was able to get an almost a perfect fit after mean-encoding the count feature?","metadata":{}},{"cell_type":"code","source":"# View the solution (Run this cell to receive credit!)\nq_3.check()","metadata":{"lines_to_next_cell":0,"execution":{"iopub.status.busy":"2021-06-18T15:15:08.709188Z","iopub.execute_input":"2021-06-18T15:15:08.709729Z","iopub.status.idle":"2021-06-18T15:15:08.717188Z","shell.execute_reply.started":"2021-06-18T15:15:08.709697Z","shell.execute_reply":"2021-06-18T15:15:08.716165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Uncomment this if you'd like a hint before seeing the answer\n#q_3.hint()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T15:15:12.803071Z","iopub.execute_input":"2021-06-18T15:15:12.803526Z","iopub.status.idle":"2021-06-18T15:15:12.807591Z","shell.execute_reply.started":"2021-06-18T15:15:12.80348Z","shell.execute_reply":"2021-06-18T15:15:12.80656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The End #\n\nThat's it for *Feature Engineering*! We hope you enjoyed your time with us.\n\nNow, are you ready to try out your new skills? Now would be a great time to join our [Housing Prices](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) Getting Started competition. We've even prepared a [Bonus Lesson]() that collects all the work we've done together into a starter notebook.\n\n# References #\nHere are some great resources you might like to consult for more information. They all played a part in shaping this course:\n- *The Art of Feature Engineering*, a book by Pablo Duboue.\n- *An Empirical Analysis of Feature Engineering for Predictive Modeling*, an article by Jeff Heaton.\n- *Feature Engineering for Machine Learning*, a book by Alice Zheng and Amanda Casari. The tutorial on clustering was inspired by this excellent book.\n- *Feature Engineering and Selection*, a book by Max Kuhn and Kjell Johnson.","metadata":{}},{"cell_type":"markdown","source":"---\n\n\n\n\n*Have questions or comments? Visit the [Learn Discussion forum](https://www.kaggle.com/learn-forum/221677) to chat with other Learners.*","metadata":{}}]}